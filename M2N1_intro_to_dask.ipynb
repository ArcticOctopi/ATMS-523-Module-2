{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATMS 523 - Module 2\n",
    "## Introduction to `dask`\n",
    "\n",
    "We all want faster computation, right?  Most computers today are multi-threaded machines that can perform computations in parallel.  Even a $1000 laptop can have 6 or 8 CPU cores that can be used to speed up processing.  It's easy to take advantage of the CPUs in these computers to accelerate computations by many times.  In addition, modern servers can have 32-64 CPU cores or more per machine, and high-performance computing clusters can allow scaling up to hundreds and thousands of processors.  In python, one package that can handle scaling computational tasks across different types of computational platorms is `dask`.  \n",
    "\n",
    "`dask` was developed to enable a simple interface to a library that can allow for distributed memory calculations across an arbitrary number of CPUs on one or many machines (through the `dask-distributed` package).  This allows for doing calculations on large datasets that use common `python` tools such as `numpy`, `pandas`, and `xarray`, with only minor changes in syntax.  Large datasets that are larger that are bigger than the random access memory (RAM) are called \"out of memory calculations\".  `dask` has the ability to automatically or manually \"chunk\" large arrays into pieces that can fit into the RAM of an individual machine or machine(s), enabling an easier pathway to solving big data problems.\n",
    "\n",
    "In this exercise we will demonstrate the use of `dask` on a local machine (a relatively modest MacBook Pro) to see the benefits of using `dask` in your workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### installing `dask`\n",
    "\n",
    "First, `dask` can be installed with\n",
    "\n",
    "`conda install dask`\n",
    "\n",
    "Make sure you are installing in the conda environment you expect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `dask` on your local machine\n",
    "in your `python` environment.  Once `dask` is installed, you can import it into your `python` session with the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "import dask\n",
    "# add if you have permissions problems on your temporary directory\n",
    "# dask.config.set(temporary_directory='/tmp/snesbitt-dask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a `dask` local client object to start the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 03:01:27,739 - distributed.scheduler - WARNING - Worker failed to heartbeat for 39933s; attempting restart: <WorkerState 'tcp://127.0.0.1:62745', name: 2, status: running, memory: 0, processing: 0>\n",
      "2025-09-22 03:01:27,740 - distributed.scheduler - WARNING - Worker failed to heartbeat for 39933s; attempting restart: <WorkerState 'tcp://127.0.0.1:62748', name: 3, status: running, memory: 0, processing: 0>\n",
      "2025-09-22 03:01:27,741 - distributed.scheduler - WARNING - Worker failed to heartbeat for 39933s; attempting restart: <WorkerState 'tcp://127.0.0.1:62753', name: 1, status: running, memory: 0, processing: 0>\n",
      "2025-09-22 03:01:27,743 - distributed.scheduler - WARNING - Worker failed to heartbeat for 39933s; attempting restart: <WorkerState 'tcp://127.0.0.1:62756', name: 0, status: running, memory: 0, processing: 0>\n",
      "2025-09-22 03:01:28,948 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-09-22 03:01:28,975 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-09-22 03:01:28,985 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-09-22 03:01:28,994 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "client = Client()  # set up local cluster on your laptop\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you expand the arrow on the output of the Client object, you can see:\n",
    "\n",
    "![Dask Cluster](dask_cluster.png)\n",
    "\n",
    "The Client tells us that there is a dashboard running at a web address that you can click on, and it also tells us the resources that we have available: 8 threads, 4 workers, and 16 gigabytes of RAM.  These are the characteristics of this laptop.  However, `dask` will allow us to more fully use the capabilities of this machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started with `dask`\n",
    "\n",
    "Once we have a `dask` Client, we can use it to submit computations.  Let's get started with something simple.\n",
    "\n",
    "#### `dask` demo\n",
    "\n",
    "Let's start with a simple calculation.  Let's say we need to compute the negative of the square root of an array.  We could easily do this with `numpy` for a small array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an array with $10^9$ elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.zeros([1000,1000,1000])\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should work fine on most machines.  Now, let's try making an array with $10^{16}$ elements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.zeros([1000,1000,1000,1000,1000])\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this did not go so well.  My machine has 16 GB of physical RAM, plus some virtual swap memory on the hard drive (Max 1 TB), but this is more than my computer can provide.  What can we do at this point?  Well, in the old days, we would split this task up into smaller tasks, and then recombine, but this takes time and sometimes a lot of effort. \n",
    "\n",
    "Is there a way to do this in a way where we can split this computation up such that we can work on datasets that are larger than the memory capabilities of a machine?  Yes!\n",
    "\n",
    "Let's swap out `numpy` for calls in the package `dask`, which streamlines and accellerates many large data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "images = da.zeros([1000,1000,1000,1000,1000])\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see that `dask` has split this problem up into chunks of `numpy` arrays, each of which can be split into available memory, and even on to available CPUs on the machine.  This both solves the memory problem and can enable parallel computation on large arrays, which should save us consierable time writing code to break this problem down into smaller ones.\n",
    "\n",
    "Let's try a quick solution to a problem of processing a large dataset.\n",
    "\n",
    "Install `gcsfs` to perform this calculation (i.e., `conda install gcsfs` in your class conda environemnt), where the data resides on the in a Google Cloud S3 bucket (one way public datasets are shared).  Also, make sure that the `dask distributed` client is running above!\n",
    "\n",
    "Also make sure `ipywidgets` is installed, so you have a fancy progress bar!\n",
    "\n",
    "This data is a table in csv format, so we will just use `pandas` to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "# browse here: https://console.cloud.google.com/storage/browser/anaconda-public-data/nyc-taxi/csv;tab=objects?prefix=&forceOnObjectsSortingFiltering=false\n",
    "\n",
    "df = dd.read_csv('gcs://anaconda-public-data/nyc-taxi/csv/2015/yellow_tripdata_2015-02.csv',\n",
    "                 storage_options={'token': 'anon'}, dtype={'extra': 'float64',\n",
    "                'tolls_amount': 'float64'},\n",
    "                 parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])[['tip_amount','fare_amount']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()\n",
    "progress(df, notebook=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute how many rows are in the dask dataframe, and in the following cell, how many passengers took taxis in 2015 in New York City.  Note that this could take some time, depending on your internet connection.  You can watch the progress of the computation if you go to the \"Dashboard Address\" that is listed when you started your `dask` cluster above.\n",
    "\n",
    "How many passengers rode NYC taxis in 2015?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.passenger_count.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[(df.tip_amount > 0) & (df.fare_amount > 0)]\n",
    "df2['tip_fraction'] = df2.tip_amount / df2.fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['tip_amount'].max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group df.tpep_pickup_datetime by dayofweek and hour\n",
    "dayofweek = df2.groupby(df2.tpep_pickup_datetime.dt.dayofweek).tip_fraction.mean() \n",
    "hour = df2.groupby(df2.tpep_pickup_datetime.dt.hour).tip_fraction.mean()\n",
    "\n",
    "dayofweek, hour = dask.persist(dayofweek, hour)\n",
    "progress(dayofweek, hour, notebook=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayofweek.compute().plot(figsize=(10, 6), title='Tip Fraction by Day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this is done, you can see that with minimal coding, you can access a large cloud dataset, process it in parallel, and with minimal coding!  And you discovered that people are either really generous in NYC late at night, or they have trouble counting their money!\n",
    "\n",
    "When you are done with your dask cluster, you can close it to recover the resources it used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Module2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
